{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5253ddc3",
   "metadata": {},
   "source": [
    "# $$ Building \\ a \\ Question \\ and \\ Answer \\ system \\ with \\ RAG $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e619ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f110f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-generativeai\n",
      "Version: 0.5.2\n",
      "Summary: Google Generative AI High level API client library and tools.\n",
      "Home-page: https://github.com/google/generative-ai-python\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\kalag\\anaconda3\\lib\\site-packages\n",
      "Requires: google-ai-generativelanguage, google-api-core, google-api-python-client, google-auth, protobuf, pydantic, tqdm, typing-extensions\n",
      "Required-by: langchain-google-genai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\kalag\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip show google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcbaeb",
   "metadata": {},
   "source": [
    "## Initializing GEMINI API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8451ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6ef94",
   "metadata": {},
   "source": [
    "## 1.Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95f91a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1d42607",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('Data/leaveNoContextBehind.pdf')\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7df6ac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate \\nVVConcat Concat \\nQ V\\nQ V\\nQs{KV}sCompressive memory & \\nLinear attention Causal scaled dot-product \\nattention & PE Linear \\nprojection \\n{KV}s-1Retrieve \\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1and{KV}sare atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.The attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024', metadata={'source': 'Data/leaveNoContextBehind.pdf', 'page': 0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dce527",
   "metadata": {},
   "source": [
    "## 2.Splitting the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84565073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "splt_text = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500,chunk_overlap=100)\n",
    "chunk_texts = splt_text.split_documents(pages)\n",
    "print(len(chunk_texts))\n",
    "print(type(chunk_texts))\n",
    "print(type(chunk_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54eb1592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate \\nVVConcat Concat \\nQ V\\nQ V\\nQs{KV}sCompressive memory & \\nLinear attention Causal scaled dot-product \\nattention & PE Linear \\nprojection \\n{KV}s-1Retrieve \\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1and{KV}sare atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.The attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024', metadata={'source': 'Data/leaveNoContextBehind.pdf', 'page': 0}),\n",
       " Document(page_content='Preprint. Under review.\\nIn this work, we introduce a novel approach that enables Transformer LLMs to effectively\\nprocess infinitely long inputs with bounded memory footprint and computation. A key\\ncomponent in our proposed approach is a new attention technique dubbed Infini-attention\\n(Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention\\nmechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local\\nattention and long-term linear attention mechanisms in a single Transformer block.\\nSuch a subtle but critical modification to the Transformer attention layer enables a natural\\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\\ntuning.\\nOur Infini-attention reuses all the key, value and query states of the standard attention\\ncomputation for long-term memory consolidation and retrieval. We store old KV states of\\nthe attention in the compressive memory, instead of discarding them like in the standard\\nattention mechanism. We then retrieve the values from the memory by using the attention\\nquery states when processing subsequent sequences. To compute the final contextual\\noutput, the Infini-attention aggregates the long-term memory-retrieved values and the local\\nattention contexts.\\nIn our experiments, we show that our approach outperforms baseline models on long-\\ncontext language modeling benchmarks while having 114x comprehension ratio in terms of\\nmemory size. The model achieves even better perplexity when trained with 100K sequence\\nlength. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval\\ntask when injected with Infini-attention. Finally, we show that a 8B model with Infini-\\nattention reaches a new SOTA result on a 500K length book summarization task after\\ncontinual pre-training and task fine-tuning.\\nIn summary, our work makes the following contributions:\\n1.We introduce a practical and yet powerful attention mechanism – Infini-attention\\nwith long-term compressive memory and local causal attention for efficiently mod-\\neling both long and short-range contextual dependencies.\\n2.Infini-attention introduces minimal change to the standard scaled dot-product atten-\\ntion and supports plug-and-play continual pre-training and long-context adaptation\\nby design.\\n3.Our approach enables Transformer LLMs to scale to infinitely long context with a\\nbounded memory and compute resource by processing extremely long inputs in a\\nstreaming fashion.\\n2 Method\\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019).\\nSimilar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We\\ncompute the standard causal dot-product attention context within each segment. So the\\ndot-product attention computation is local in a sense that it covers a total Nnumber of\\ntokens of the current segment with index S(Nis the segment length).\\nThe local attention (Dai et al., 2019), however, discards the attention states of the previous\\nsegment when processing the next one. In Infini-Transformers, instead of leaving out the\\nold KV attention states, we propose to reuse them to maintain the entire context history\\nwith a compressive memory. So each attention layer of Infini-Transformers has both global\\ncompressive and local fine-grained states. We call such an efficient attention mechanism\\nInfini-attention, which is illustrated in Figure 1 and described formally in the following\\nsections.\\n2.1 Infini-attention\\nAs shown Figure 1, our Infini-attention computes both local and global context states and\\ncombine them for its output. Similar to multi-head attention (MHA), it maintains Hnumber\\n2', metadata={'source': 'Data/leaveNoContextBehind.pdf', 'page': 1}),\n",
       " Document(page_content='Preprint. Under review.\\nSegment 1 Segment 2 Segment 3 \\nSegment 1 Segment 2 Segment 3 Transformer block: Infini-T ransformer \\nT ransformer-XL Compressive memory: \\nMemory update: \\nMemory retrieval: \\nEffective context: \\nInput segment: Segment 1 \\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL\\n(bottom) discards old contexts since it caches the KV states for the last segment only.\\nof parallel compressive memory per attention layer ( His the number of attention heads) in\\naddition to the dot-product attention.\\n2.1.1 Scaled Dot-product Attention\\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention\\nvariant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in\\nLLMs. The MHA’s strong capability to model context-dependent dynamic computation and\\nits conveniences of temporal masking have been leveraged extensively in the autoregressive\\ngenerative models.\\nA single head in the vanilla MHA computes its attention context Adot∈I RN×dvaluefrom\\nsequence of input segments X∈I RN×dmodel as follows. First, it computes attention query,\\nkey, and value states:\\nK=XW K,V=XW Vand Q=XW Q. (1)\\nHere, WK∈I Rdmodel×dkey,WV∈I Rdmodel×dvalueand WQ∈I Rdmodel×dkeyare trainable projection\\nmatrices. Then, the attention context is calculated as a weighted average of all other values\\nas\\nAdot=softmax\\x12QKT\\n√dmodel\\x13\\nV. (2)\\nFor MHA, we compute Hnumber of attention context vectors for each sequence element\\nin parallel, concatenate them along the second dimension and then finally project the\\nconcatenated vector to the model space to obtain attention the output.\\n2.1.2 Compressive Memory\\nIn Infini-attention, instead of computing new memory entries for compressive memory, we\\nreuse the query, key and value states ( Q,Kand V) from the dot-product attention compu-\\ntation. The state sharing and reusing between the dot-product attention and compressive\\nmemory not only enables efficient plug-in-play long-context adaptation but also speeds up\\ntraining and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to\\nstore bindings of key and value states in the compressive memory and retrieve by using the\\nquery vectors.\\n3', metadata={'source': 'Data/leaveNoContextBehind.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_texts[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0506fec",
   "metadata": {},
   "source": [
    "## 3.Create Chunks Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62518ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(google_api_key=GEMINI_API_KEY,\n",
    "                                          model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f0ec2",
   "metadata": {},
   "source": [
    "## 4.Store the chunks in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba4a0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunk_texts, embeddings, persist_directory=\"vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e6ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "274844eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, \n",
    "                                   model=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19a26a",
   "metadata": {},
   "source": [
    "## 5.Setup the PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e8eabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    " \n",
    "    SystemMessage(content=\"\"\"Please answer the following question as thoroughly as possible using the provided context.\n",
    "    If the context does not contain the answer, simply state,\n",
    "    'The answer is not available in the context.' Avoid guessing or providing incorrect answers.\"\"\"),\n",
    "    \n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Aswer the question based on the given context.\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: \n",
    "    {question}\n",
    "    \n",
    "    Answer: \"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0c523",
   "metadata": {},
   "source": [
    "## 5.Based on users query retrieve the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "151a295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = ({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "             | chat_template | chat_model| output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bdcf6",
   "metadata": {},
   "source": [
    "## 6.Pass the context and question to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dedd8b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Transformer Summary:\\n\\n* **Attention Mechanism:** Transformers utilize attention mechanisms, allowing them to focus on relevant parts of the input sequence when making predictions. This is in contrast to traditional recurrent neural networks that process sequences sequentially.\\n* **Long-Context Challenges:**  While powerful, transformers face challenges with long sequences due to limitations in memory and computational efficiency. Researchers are exploring techniques like compressive memory, efficient attention mechanisms, and long-context continual pre-training to address these issues.\\n* **Scaling Transformers:**  Efforts are underway to scale transformers to handle even longer sequences, such as millions or even billions of tokens. This involves developing new architectures, data engineering techniques, and efficient attention mechanisms.\\n* **Applications:**  Transformers have revolutionized natural language processing tasks, excelling in areas like machine translation, text summarization, and question answering. They are also being applied to other domains, including computer vision and speech recognition.\\n* **Ongoing Research:**  Active research continues to improve transformer efficiency, scalability, and performance. This includes exploring new architectures, training methods, and ways to better handle long-range dependencies in sequences. \\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Write Summary in 5 lines about transformers\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c54965d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Key Contribution of Infini-attention\n",
       "\n",
       "The key contribution of the Infini-attention mechanism, as described in the context, is its ability to enable Transformer LLMs (Large Language Models) to efficiently process infinitely long inputs while maintaining a bounded memory footprint and computational cost. This is achieved through a combination of:\n",
       "\n",
       "* **Compressive Memory:** Infini-attention incorporates a compressive memory that stores old key-value (KV) attention states, unlike standard attention mechanisms that discard them. This allows the model to retain and access long-term context history.\n",
       "* **Local and Global Context:** The mechanism combines both masked local attention for focusing on recent context within a segment and long-term linear attention for retrieving information from the compressive memory. This enables efficient modeling of both short-range and long-range dependencies.\n",
       "\n",
       "Essentially, Infini-attention allows LLMs to scale to infinitely long contexts without the typical memory and computational constraints, making it a significant advancement in the field. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is the key contribution of the Infini-attention mechanism proposed in the paper?\")\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b3ea3afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The answer is not available in the context. \n",
       "    The provided text focuses on research in natural language processing, specifically attention mechanisms and memory networks, and does not contain information about diabetes. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Write Summary in 5 lines about diabetes\")\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543c0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
